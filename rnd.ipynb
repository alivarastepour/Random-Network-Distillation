{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alivarastepour/Random-Network-Distillation/blob/master/rnd.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H1kNpbL61hTR",
        "outputId": "a8bfcfc3-444f-4f27-a318-65c2306d5245"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "30.26s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
            "  File \"<frozen runpy>\", line 88, in _run_code\n",
            "  File \"/var/data/python/lib/python3.12/site-packages/pip/__main__.py\", line 21, in <module>\n",
            "    from pip._internal.cli.main import main as _main\n",
            "  File \"/var/data/python/lib/python3.12/site-packages/pip/_internal/cli/main.py\", line 8, in <module>\n",
            "    from pip._internal.cli.autocompletion import autocomplete\n",
            "  File \"/var/data/python/lib/python3.12/site-packages/pip/_internal/cli/autocompletion.py\", line 9, in <module>\n",
            "    from pip._internal.cli.main_parser import create_main_parser\n",
            "  File \"/var/data/python/lib/python3.12/site-packages/pip/_internal/cli/main_parser.py\", line 7, in <module>\n",
            "    from pip._internal.cli import cmdoptions\n",
            "  File \"/var/data/python/lib/python3.12/site-packages/pip/_internal/cli/cmdoptions.py\", line 22, in <module>\n",
            "    from pip._internal.cli.progress_bars import BAR_TYPES\n",
            "  File \"/var/data/python/lib/python3.12/site-packages/pip/_internal/cli/progress_bars.py\", line 9, in <module>\n",
            "    from pip._internal.utils.logging import get_indentation\n",
            "  File \"/var/data/python/lib/python3.12/site-packages/pip/_internal/utils/logging.py\", line 14, in <module>\n",
            "    from pip._internal.utils.misc import ensure_dir\n",
            "  File \"/var/data/python/lib/python3.12/site-packages/pip/_internal/utils/misc.py\", line 20, in <module>\n",
            "    from pip._vendor import pkg_resources\n",
            "  File \"/var/data/python/lib/python3.12/site-packages/pip/_vendor/pkg_resources/__init__.py\", line 58, in <module>\n",
            "    from pip._vendor.six.moves import urllib, map, filter\n",
            "ModuleNotFoundError: No module named 'pip._vendor.six.moves'\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install \"gym[atari, accept-rom-license]\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sS0mupXL0l5A",
        "outputId": "58d4f51b-de7e-483f-9a8b-d439aa441bb9"
      },
      "outputs": [],
      "source": [
        "%pip install gym_super_mario_bros"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "al473NOO1YED",
        "outputId": "b1581584-9063-4786-9a07-f170afdddfa7"
      },
      "outputs": [],
      "source": [
        "!pip install tensorboardX"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "utqZMtwTz5E5"
      },
      "outputs": [],
      "source": [
        "import configparser\n",
        "\n",
        "config = configparser.ConfigParser()\n",
        "config.read('./config.conf')\n",
        "\n",
        "# ---------------------------------\n",
        "default = 'DEFAULT'\n",
        "# ---------------------------------\n",
        "default_config = config[default]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "Q8ubpluwzdXH"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "from torch import inf\n",
        "# if default_config['TrainMethod'] in ['PPO', 'ICM', 'RND']:\n",
        "#     num_step = int(ppo_config['NumStep'])\n",
        "# else:\n",
        "#     num_step = int(default_config['NumStep'])\n",
        "\n",
        "use_gae = default_config.getboolean('UseGAE')\n",
        "lam = float(default_config['Lambda'])\n",
        "train_method = default_config['TrainMethod']\n",
        "\n",
        "\n",
        "def make_train_data(reward, done, value, gamma, num_step, num_worker):\n",
        "    discounted_return = np.empty([num_worker, num_step])\n",
        "\n",
        "    # Discounted Return\n",
        "    if use_gae:\n",
        "        gae = np.zeros_like([num_worker, ])\n",
        "        for t in range(num_step - 1, -1, -1):\n",
        "            delta = reward[:, t] + gamma * value[:, t + 1] * (1 - done[:, t]) - value[:, t]\n",
        "            gae = delta + gamma * lam * (1 - done[:, t]) * gae\n",
        "\n",
        "            discounted_return[:, t] = gae + value[:, t]\n",
        "\n",
        "            # For Actor\n",
        "        adv = discounted_return - value[:, :-1]\n",
        "\n",
        "    else:\n",
        "        running_add = value[:, -1]\n",
        "        for t in range(num_step - 1, -1, -1):\n",
        "            running_add = reward[:, t] + gamma * running_add * (1 - done[:, t])\n",
        "            discounted_return[:, t] = running_add\n",
        "\n",
        "        # For Actor\n",
        "        adv = discounted_return - value[:, :-1]\n",
        "\n",
        "    return discounted_return.reshape([-1]), adv.reshape([-1])\n",
        "\n",
        "\n",
        "class RunningMeanStd(object):\n",
        "    # https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance#Parallel_algorithm\n",
        "    def __init__(self, epsilon=1e-4, shape=()):\n",
        "        self.mean = np.zeros(shape, 'float64')\n",
        "        self.var = np.ones(shape, 'float64')\n",
        "        self.count = epsilon\n",
        "\n",
        "    def update(self, x):\n",
        "        batch_mean = np.mean(x, axis=0)\n",
        "        batch_var = np.var(x, axis=0)\n",
        "        batch_count = x.shape[0]\n",
        "        self.update_from_moments(batch_mean, batch_var, batch_count)\n",
        "\n",
        "    def update_from_moments(self, batch_mean, batch_var, batch_count):\n",
        "        delta = batch_mean - self.mean\n",
        "        tot_count = self.count + batch_count\n",
        "\n",
        "        new_mean = self.mean + delta * batch_count / tot_count\n",
        "        m_a = self.var * (self.count)\n",
        "        m_b = batch_var * (batch_count)\n",
        "        M2 = m_a + m_b + np.square(delta) * self.count * batch_count / (self.count + batch_count)\n",
        "        new_var = M2 / (self.count + batch_count)\n",
        "\n",
        "        new_count = batch_count + self.count\n",
        "\n",
        "        self.mean = new_mean\n",
        "        self.var = new_var\n",
        "        self.count = new_count\n",
        "\n",
        "\n",
        "class RewardForwardFilter(object):\n",
        "    def __init__(self, gamma):\n",
        "        self.rewems = None\n",
        "        self.gamma = gamma\n",
        "\n",
        "    def update(self, rews):\n",
        "        if self.rewems is None:\n",
        "            self.rewems = rews\n",
        "        else:\n",
        "            self.rewems = self.rewems * self.gamma + rews\n",
        "        return self.rewems\n",
        "\n",
        "\n",
        "def softmax(z):\n",
        "    assert len(z.shape) == 2\n",
        "    s = np.max(z, axis=1)\n",
        "    s = s[:, np.newaxis]  # necessary step to do broadcasting\n",
        "    e_x = np.exp(z - s)\n",
        "    div = np.sum(e_x, axis=1)\n",
        "    div = div[:, np.newaxis]  # dito\n",
        "    return e_x / div\n",
        "\n",
        "\n",
        "def global_grad_norm_(parameters, norm_type=2):\n",
        "    r\"\"\"Clips gradient norm of an iterable of parameters.\n",
        "\n",
        "    The norm is computed over all gradients together, as if they were\n",
        "    concatenated into a single vector. Gradients are modified in-place.\n",
        "\n",
        "    Arguments:\n",
        "        parameters (Iterable[Tensor] or Tensor): an iterable of Tensors or a\n",
        "            single Tensor that will have gradients normalized\n",
        "        max_norm (float or int): max norm of the gradients\n",
        "        norm_type (float or int): type of the used p-norm. Can be ``'inf'`` for\n",
        "            infinity norm.\n",
        "\n",
        "    Returns:\n",
        "        Total norm of the parameters (viewed as a single vector).\n",
        "    \"\"\"\n",
        "    if isinstance(parameters, torch.Tensor):\n",
        "        parameters = [parameters]\n",
        "    parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
        "    norm_type = float(norm_type)\n",
        "    if norm_type == inf:\n",
        "        total_norm = max(p.grad.data.abs().max() for p in parameters)\n",
        "    else:\n",
        "        total_norm = 0\n",
        "        for p in parameters:\n",
        "            param_norm = p.grad.data.norm(norm_type)\n",
        "            total_norm += param_norm.item() ** norm_type\n",
        "        total_norm = total_norm ** (1. / norm_type)\n",
        "\n",
        "    return total_norm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "b8QaWFRVzlLu"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import math\n",
        "from torch.nn import init\n",
        "\n",
        "\n",
        "class NoisyLinear(nn.Module):\n",
        "    \"\"\"Factorised Gaussian NoisyNet\"\"\"\n",
        "\n",
        "    def __init__(self, in_features, out_features, sigma0=0.5):\n",
        "        super().__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.weight = nn.Parameter(torch.Tensor(out_features, in_features))\n",
        "        self.bias = nn.Parameter(torch.Tensor(out_features))\n",
        "        self.noisy_weight = nn.Parameter(\n",
        "            torch.Tensor(out_features, in_features))\n",
        "        self.noisy_bias = nn.Parameter(torch.Tensor(out_features))\n",
        "        self.noise_std = sigma0 / math.sqrt(self.in_features)\n",
        "\n",
        "        self.reset_parameters()\n",
        "        self.register_noise()\n",
        "\n",
        "    def register_noise(self):\n",
        "        in_noise = torch.FloatTensor(self.in_features)\n",
        "        out_noise = torch.FloatTensor(self.out_features)\n",
        "        noise = torch.FloatTensor(self.out_features, self.in_features)\n",
        "        self.register_buffer('in_noise', in_noise)\n",
        "        self.register_buffer('out_noise', out_noise)\n",
        "        self.register_buffer('noise', noise)\n",
        "\n",
        "    def sample_noise(self):\n",
        "        self.in_noise.normal_(0, self.noise_std)\n",
        "        self.out_noise.normal_(0, self.noise_std)\n",
        "        self.noise = torch.mm(\n",
        "            self.out_noise.view(-1, 1), self.in_noise.view(1, -1))\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
        "        self.weight.data.uniform_(-stdv, stdv)\n",
        "        self.noisy_weight.data.uniform_(-stdv, stdv)\n",
        "        if self.bias is not None:\n",
        "            self.bias.data.uniform_(-stdv, stdv)\n",
        "            self.noisy_bias.data.uniform_(-stdv, stdv)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Note: noise will be updated if x is not volatile\n",
        "        \"\"\"\n",
        "        normal_y = nn.functional.linear(x, self.weight, self.bias)\n",
        "        if self.training:\n",
        "            # update the noise once per update\n",
        "            self.sample_noise()\n",
        "\n",
        "        noisy_weight = self.noisy_weight * self.noise\n",
        "        noisy_bias = self.noisy_bias * self.out_noise\n",
        "        noisy_y = nn.functional.linear(x, noisy_weight, noisy_bias)\n",
        "        return noisy_y + normal_y\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__ + '(' \\\n",
        "            + 'in_features=' + str(self.in_features) \\\n",
        "            + ', out_features=' + str(self.out_features) + ')'\n",
        "\n",
        "\n",
        "class Flatten(nn.Module):\n",
        "    def forward(self, input):\n",
        "        return input.view(input.size(0), -1)\n",
        "\n",
        "\n",
        "class CnnActorCriticNetwork(nn.Module):\n",
        "    def __init__(self, input_size, output_size, use_noisy_net=False):\n",
        "        super(CnnActorCriticNetwork, self).__init__()\n",
        "\n",
        "        if use_noisy_net:\n",
        "            print('use NoisyNet')\n",
        "            linear = NoisyLinear\n",
        "        else:\n",
        "            linear = nn.Linear\n",
        "\n",
        "        self.feature = nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_channels=4,\n",
        "                out_channels=32,\n",
        "                kernel_size=8,\n",
        "                stride=4),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(\n",
        "                in_channels=32,\n",
        "                out_channels=64,\n",
        "                kernel_size=4,\n",
        "                stride=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(\n",
        "                in_channels=64,\n",
        "                out_channels=64,\n",
        "                kernel_size=3,\n",
        "                stride=1),\n",
        "            nn.ReLU(),\n",
        "            Flatten(),\n",
        "            linear(\n",
        "                7 * 7 * 64,\n",
        "                256),\n",
        "            nn.ReLU(),\n",
        "            linear(\n",
        "                256,\n",
        "                448),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.actor = nn.Sequential(\n",
        "            linear(448, 448),\n",
        "            nn.ReLU(),\n",
        "            linear(448, output_size)\n",
        "        )\n",
        "\n",
        "        self.extra_layer = nn.Sequential(\n",
        "            linear(448, 448),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.critic_ext = linear(448, 1)\n",
        "        self.critic_int = linear(448, 1)\n",
        "\n",
        "        for p in self.modules():\n",
        "            if isinstance(p, nn.Conv2d):\n",
        "                init.orthogonal_(p.weight, np.sqrt(2))\n",
        "                p.bias.data.zero_()\n",
        "\n",
        "            if isinstance(p, nn.Linear):\n",
        "                init.orthogonal_(p.weight, np.sqrt(2))\n",
        "                p.bias.data.zero_()\n",
        "\n",
        "        init.orthogonal_(self.critic_ext.weight, 0.01)\n",
        "        self.critic_ext.bias.data.zero_()\n",
        "\n",
        "        init.orthogonal_(self.critic_int.weight, 0.01)\n",
        "        self.critic_int.bias.data.zero_()\n",
        "\n",
        "        for i in range(len(self.actor)):\n",
        "            if type(self.actor[i]) == nn.Linear:\n",
        "                init.orthogonal_(self.actor[i].weight, 0.01)\n",
        "                self.actor[i].bias.data.zero_()\n",
        "\n",
        "        for i in range(len(self.extra_layer)):\n",
        "            if type(self.extra_layer[i]) == nn.Linear:\n",
        "                init.orthogonal_(self.extra_layer[i].weight, 0.1)\n",
        "                self.extra_layer[i].bias.data.zero_()\n",
        "\n",
        "    def forward(self, state):\n",
        "        x = self.feature(state)\n",
        "        policy = self.actor(x)\n",
        "        value_ext = self.critic_ext(self.extra_layer(x) + x)\n",
        "        value_int = self.critic_int(self.extra_layer(x) + x)\n",
        "        return policy, value_ext, value_int\n",
        "\n",
        "\n",
        "class RNDModel(nn.Module):\n",
        "    def __init__(self, input_size, output_size):\n",
        "        super(RNDModel, self).__init__()\n",
        "\n",
        "        self.input_size = input_size\n",
        "        self.output_size = output_size\n",
        "\n",
        "        feature_output = 7 * 7 * 64\n",
        "        self.predictor = nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_channels=1,\n",
        "                out_channels=32,\n",
        "                kernel_size=8,\n",
        "                stride=4),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Conv2d(\n",
        "                in_channels=32,\n",
        "                out_channels=64,\n",
        "                kernel_size=4,\n",
        "                stride=2),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Conv2d(\n",
        "                in_channels=64,\n",
        "                out_channels=64,\n",
        "                kernel_size=3,\n",
        "                stride=1),\n",
        "            nn.LeakyReLU(),\n",
        "            Flatten(),\n",
        "            nn.Linear(feature_output, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 512)\n",
        "        )\n",
        "\n",
        "        self.target = nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_channels=1,\n",
        "                out_channels=32,\n",
        "                kernel_size=8,\n",
        "                stride=4),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Conv2d(\n",
        "                in_channels=32,\n",
        "                out_channels=64,\n",
        "                kernel_size=4,\n",
        "                stride=2),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Conv2d(\n",
        "                in_channels=64,\n",
        "                out_channels=64,\n",
        "                kernel_size=3,\n",
        "                stride=1),\n",
        "            nn.LeakyReLU(),\n",
        "            Flatten(),\n",
        "            nn.Linear(feature_output, 512)\n",
        "        )\n",
        "\n",
        "        for p in self.modules():\n",
        "            if isinstance(p, nn.Conv2d):\n",
        "                init.orthogonal_(p.weight, np.sqrt(2))\n",
        "                p.bias.data.zero_()\n",
        "\n",
        "            if isinstance(p, nn.Linear):\n",
        "                init.orthogonal_(p.weight, np.sqrt(2))\n",
        "                p.bias.data.zero_()\n",
        "\n",
        "        for param in self.target.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "    def forward(self, next_obs):\n",
        "        target_feature = self.target(next_obs)\n",
        "        predict_feature = self.predictor(next_obs)\n",
        "\n",
        "        return predict_feature, target_feature"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "s370OxwlzTdH"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "\n",
        "from torch.distributions.categorical import Categorical\n",
        "\n",
        "\n",
        "\n",
        "class RNDAgent(object):\n",
        "    def __init__(\n",
        "            self,\n",
        "            input_size,\n",
        "            output_size,\n",
        "            num_env,\n",
        "            num_step,\n",
        "            gamma,\n",
        "            lam=0.95,\n",
        "            learning_rate=1e-4,\n",
        "            ent_coef=0.01,\n",
        "            clip_grad_norm=0.5,\n",
        "            epoch=3,\n",
        "            batch_size=128,\n",
        "            ppo_eps=0.1,\n",
        "            update_proportion=0.25,\n",
        "            use_gae=True,\n",
        "            use_cuda=False,\n",
        "            use_noisy_net=False):\n",
        "        self.model = CnnActorCriticNetwork(input_size, output_size, use_noisy_net)\n",
        "        self.num_env = num_env\n",
        "        self.output_size = output_size\n",
        "        self.input_size = input_size\n",
        "        self.num_step = num_step\n",
        "        self.gamma = gamma\n",
        "        self.lam = lam\n",
        "        self.epoch = epoch\n",
        "        self.batch_size = batch_size\n",
        "        self.use_gae = use_gae\n",
        "        self.ent_coef = ent_coef\n",
        "        self.ppo_eps = ppo_eps\n",
        "        self.clip_grad_norm = clip_grad_norm\n",
        "        self.update_proportion = update_proportion\n",
        "        self.device = torch.device('cuda' if use_cuda else 'cpu')\n",
        "\n",
        "        self.rnd = RNDModel(input_size, output_size)\n",
        "        self.optimizer = optim.Adam(list(self.model.parameters()) + list(self.rnd.predictor.parameters()),\n",
        "                                    lr=learning_rate)\n",
        "        self.rnd = self.rnd.to(self.device)\n",
        "\n",
        "        self.model = self.model.to(self.device)\n",
        "\n",
        "    def get_action(self, state):\n",
        "        state = torch.Tensor(state).to(self.device)\n",
        "        state = state.float()\n",
        "        policy, value_ext, value_int = self.model(state)\n",
        "        action_prob = F.softmax(policy, dim=-1).data.cpu().numpy()\n",
        "\n",
        "        action = self.random_choice_prob_index(action_prob)\n",
        "\n",
        "        return action, value_ext.data.cpu().numpy().squeeze(), value_int.data.cpu().numpy().squeeze(), policy.detach()\n",
        "\n",
        "    @staticmethod\n",
        "    def random_choice_prob_index(p, axis=1):\n",
        "        r = np.expand_dims(np.random.rand(p.shape[1 - axis]), axis=axis)\n",
        "        return (p.cumsum(axis=axis) > r).argmax(axis=axis)\n",
        "\n",
        "    def compute_intrinsic_reward(self, next_obs):\n",
        "        next_obs = torch.FloatTensor(next_obs).to(self.device)\n",
        "\n",
        "        target_next_feature = self.rnd.target(next_obs)\n",
        "        predict_next_feature = self.rnd.predictor(next_obs)\n",
        "        intrinsic_reward = (target_next_feature - predict_next_feature).pow(2).sum(1) / 2\n",
        "\n",
        "        return intrinsic_reward.data.cpu().numpy()\n",
        "\n",
        "    def train_model(self, s_batch, target_ext_batch, target_int_batch, y_batch, adv_batch, next_obs_batch, old_policy):\n",
        "        s_batch = torch.FloatTensor(s_batch).to(self.device)\n",
        "        target_ext_batch = torch.FloatTensor(target_ext_batch).to(self.device)\n",
        "        target_int_batch = torch.FloatTensor(target_int_batch).to(self.device)\n",
        "        y_batch = torch.LongTensor(y_batch).to(self.device)\n",
        "        adv_batch = torch.FloatTensor(adv_batch).to(self.device)\n",
        "        next_obs_batch = torch.FloatTensor(next_obs_batch).to(self.device)\n",
        "\n",
        "        sample_range = np.arange(len(s_batch))\n",
        "        forward_mse = nn.MSELoss(reduction='none')\n",
        "\n",
        "        with torch.no_grad():\n",
        "            policy_old_list = torch.stack(old_policy).permute(1, 0, 2).contiguous().view(-1, self.output_size).to(\n",
        "                self.device)\n",
        "\n",
        "            m_old = Categorical(F.softmax(policy_old_list, dim=-1))\n",
        "            log_prob_old = m_old.log_prob(y_batch)\n",
        "            # ------------------------------------------------------------\n",
        "\n",
        "        for i in range(self.epoch):\n",
        "            np.random.shuffle(sample_range)\n",
        "            for j in range(int(len(s_batch) / self.batch_size)):\n",
        "                sample_idx = sample_range[self.batch_size * j:self.batch_size * (j + 1)]\n",
        "\n",
        "                # --------------------------------------------------------------------------------\n",
        "                # for Curiosity-driven(Random Network Distillation)\n",
        "                predict_next_state_feature, target_next_state_feature = self.rnd(next_obs_batch[sample_idx])\n",
        "\n",
        "                forward_loss = forward_mse(predict_next_state_feature, target_next_state_feature.detach()).mean(-1)\n",
        "                # Proportion of exp used for predictor update\n",
        "                mask = torch.rand(len(forward_loss)).to(self.device)\n",
        "                mask = (mask < self.update_proportion).type(torch.FloatTensor).to(self.device)\n",
        "                forward_loss = (forward_loss * mask).sum() / torch.max(mask.sum(), torch.Tensor([1]).to(self.device))\n",
        "                # ---------------------------------------------------------------------------------\n",
        "\n",
        "                policy, value_ext, value_int = self.model(s_batch[sample_idx])\n",
        "                m = Categorical(F.softmax(policy, dim=-1))\n",
        "                log_prob = m.log_prob(y_batch[sample_idx])\n",
        "\n",
        "                ratio = torch.exp(log_prob - log_prob_old[sample_idx])\n",
        "\n",
        "                surr1 = ratio * adv_batch[sample_idx]\n",
        "                surr2 = torch.clamp(\n",
        "                    ratio,\n",
        "                    1.0 - self.ppo_eps,\n",
        "                    1.0 + self.ppo_eps) * adv_batch[sample_idx]\n",
        "\n",
        "                actor_loss = -torch.min(surr1, surr2).mean()\n",
        "                critic_ext_loss = F.mse_loss(value_ext.sum(1), target_ext_batch[sample_idx])\n",
        "                critic_int_loss = F.mse_loss(value_int.sum(1), target_int_batch[sample_idx])\n",
        "\n",
        "                critic_loss = critic_ext_loss + critic_int_loss\n",
        "\n",
        "                entropy = m.entropy().mean()\n",
        "\n",
        "                self.optimizer.zero_grad()\n",
        "                loss = actor_loss + 0.5 * critic_loss - self.ent_coef * entropy + forward_loss\n",
        "                loss.backward()\n",
        "                global_grad_norm_(list(self.model.parameters())+list(self.rnd.predictor.parameters()))\n",
        "                self.optimizer.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2PKXT1N20dKo",
        "outputId": "1ae809fd-0abc-4cd3-ddf7-950ed349c1cb"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "import cv2\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from abc import abstractmethod\n",
        "from collections import deque\n",
        "from copy import copy\n",
        "\n",
        "import gym_super_mario_bros\n",
        "from nes_py.wrappers import JoypadSpace as BinarySpaceToDiscreteSpaceEnv\n",
        "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT, COMPLEX_MOVEMENT\n",
        "\n",
        "from torch.multiprocessing import Pipe, Process\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "train_method = default_config['TrainMethod']\n",
        "max_step_per_episode = int(default_config['MaxStepPerEpisode'])\n",
        "\n",
        "\n",
        "class Environment(Process):\n",
        "    @abstractmethod\n",
        "    def run(self):\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def reset(self):\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def pre_proc(self, x):\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def get_init_state(self, x):\n",
        "        pass\n",
        "\n",
        "\n",
        "def unwrap(env):\n",
        "    if hasattr(env, \"unwrapped\"):\n",
        "        return env.unwrapped\n",
        "    elif hasattr(env, \"env\"):\n",
        "        return unwrap(env.env)\n",
        "    elif hasattr(env, \"leg_env\"):\n",
        "        return unwrap(env.leg_env)\n",
        "    else:\n",
        "        return env\n",
        "\n",
        "\n",
        "class MaxAndSkipEnv(gym.Wrapper):\n",
        "    def __init__(self, env, is_render, skip=4):\n",
        "        \"\"\"Return only every `skip`-th frame\"\"\"\n",
        "        gym.Wrapper.__init__(self, env)\n",
        "        # most recent raw observations (for max pooling across time steps)\n",
        "        self._obs_buffer = np.zeros((2,) + env.observation_space.shape, dtype=np.uint8)\n",
        "        self._skip = skip\n",
        "        self.is_render = is_render\n",
        "\n",
        "    def step(self, action):\n",
        "        \"\"\"Repeat action, sum reward, and max over last observations.\"\"\"\n",
        "        total_reward = 0.0\n",
        "        done = None\n",
        "        for i in range(self._skip):\n",
        "            obs, reward, done, info = self.env.step(action) # ?\n",
        "            if self.is_render:\n",
        "                self.env.render()\n",
        "            if i == self._skip - 2:\n",
        "                self._obs_buffer[0] = obs\n",
        "            if i == self._skip - 1:\n",
        "                self._obs_buffer[1] = obs\n",
        "            total_reward += reward\n",
        "            if done:\n",
        "                break\n",
        "        # Note that the observation on the done=True frame\n",
        "        # doesn't matter\n",
        "        max_frame = self._obs_buffer.max(axis=0)\n",
        "\n",
        "        return max_frame, total_reward, done, info\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "        return self.env.reset(**kwargs)\n",
        "\n",
        "\n",
        "class MontezumaInfoWrapper(gym.Wrapper):\n",
        "    def __init__(self, env, room_address):\n",
        "        super(MontezumaInfoWrapper, self).__init__(env)\n",
        "        self.room_address = room_address\n",
        "        self.visited_rooms = set()\n",
        "\n",
        "    def get_current_room(self):\n",
        "        ram = unwrap(self.env).ale.getRAM()\n",
        "        assert len(ram) == 128\n",
        "        return int(ram[self.room_address])\n",
        "\n",
        "    def step(self, action):\n",
        "        obs, rew, done, info = self.env.step(action) # montezuma only\n",
        "        self.visited_rooms.add(self.get_current_room())\n",
        "\n",
        "        if 'episode' not in info:\n",
        "            info['episode'] = {}\n",
        "        info['episode'].update(visited_rooms=copy(self.visited_rooms))\n",
        "\n",
        "        if done:\n",
        "            self.visited_rooms.clear()\n",
        "        return obs, rew, done, info\n",
        "\n",
        "    def reset(self):\n",
        "        return self.env.reset()\n",
        "\n",
        "\n",
        "class AtariEnvironment(Environment):\n",
        "    def __init__(\n",
        "            self,\n",
        "            env_id,\n",
        "            is_render,\n",
        "            env_idx,\n",
        "            child_conn,\n",
        "            history_size=4,\n",
        "            h=84,\n",
        "            w=84,\n",
        "            life_done=True,\n",
        "            sticky_action=True,\n",
        "            p=0.25):\n",
        "        super(AtariEnvironment, self).__init__()\n",
        "        self.daemon = True\n",
        "        self.env = MaxAndSkipEnv(gym.make(env_id, render_mode=\"human\"), is_render)\n",
        "        if 'Montezuma' in env_id:\n",
        "            self.env = MontezumaInfoWrapper(self.env, room_address=3 if 'Montezuma' in env_id else 1)\n",
        "        self.env_id = env_id\n",
        "        self.is_render = is_render\n",
        "        self.env_idx = env_idx\n",
        "        self.steps = 0\n",
        "        self.episode = 0\n",
        "        self.rall = 0\n",
        "        self.recent_rlist = deque(maxlen=100)\n",
        "        self.child_conn = child_conn\n",
        "\n",
        "        self.sticky_action = sticky_action\n",
        "        self.last_action = 0\n",
        "        self.p = p\n",
        "\n",
        "        self.history_size = history_size\n",
        "        self.history = np.zeros([history_size, h, w])\n",
        "        self.h = h\n",
        "        self.w = w\n",
        "\n",
        "        self.reset()\n",
        "\n",
        "    def run(self):\n",
        "        super(AtariEnvironment, self).run()\n",
        "        while True:\n",
        "            action = self.child_conn.recv()\n",
        "\n",
        "            if 'Breakout' in self.env_id:\n",
        "                action += 1\n",
        "\n",
        "            # sticky action\n",
        "            if self.sticky_action:\n",
        "                if np.random.rand() <= self.p:\n",
        "                    action = self.last_action\n",
        "                self.last_action = action\n",
        "\n",
        "            s, reward, done, info = self.env.step(action)\n",
        "\n",
        "            if max_step_per_episode < self.steps:\n",
        "                done = True\n",
        "\n",
        "            log_reward = reward\n",
        "            force_done = done\n",
        "\n",
        "            self.history[:3, :, :] = self.history[1:, :, :]\n",
        "            self.history[3, :, :] = self.pre_proc(s)\n",
        "\n",
        "            self.rall += reward\n",
        "            self.steps += 1\n",
        "\n",
        "            if done:\n",
        "                self.recent_rlist.append(self.rall)\n",
        "                print(\"[Episode {}({})] Step: {}  Reward: {}  Recent Reward: {}  Visited Room: [{}]\".format(\n",
        "                    self.episode, self.env_idx, self.steps, self.rall, np.mean(self.recent_rlist),\n",
        "                    info.get('episode', {}).get('visited_rooms', {})))\n",
        "\n",
        "                self.history = self.reset()\n",
        "\n",
        "            self.child_conn.send(\n",
        "                [self.history[:, :, :], reward, force_done, done, log_reward])\n",
        "\n",
        "    def reset(self):\n",
        "        self.last_action = 0\n",
        "        self.steps = 0\n",
        "        self.episode += 1\n",
        "        self.rall = 0\n",
        "        s = self.env.reset()\n",
        "        self.get_init_state(\n",
        "            self.pre_proc(s))\n",
        "        return self.history[:, :, :]\n",
        "\n",
        "    def pre_proc(self, X):\n",
        "        X = np.array(Image.fromarray(X).convert('L')).astype('float32')\n",
        "        x = cv2.resize(X, (self.h, self.w))\n",
        "        return x\n",
        "\n",
        "    def get_init_state(self, s):\n",
        "        for i in range(self.history_size):\n",
        "            self.history[i, :, :] = self.pre_proc(s)\n",
        "\n",
        "\n",
        "class MarioEnvironment(Process):\n",
        "    def __init__(\n",
        "            self,\n",
        "            env_id,\n",
        "            is_render,\n",
        "            env_idx,\n",
        "            child_conn,\n",
        "            history_size=4,\n",
        "            life_done=False,\n",
        "            h=84,\n",
        "            w=84, movement=COMPLEX_MOVEMENT, sticky_action=True,\n",
        "            p=0.25):\n",
        "        super(MarioEnvironment, self).__init__()\n",
        "        self.daemon = True\n",
        "        self.env = BinarySpaceToDiscreteSpaceEnv(\n",
        "            gym_super_mario_bros.make(env_id), COMPLEX_MOVEMENT)\n",
        "\n",
        "        self.is_render = is_render\n",
        "        self.env_idx = env_idx\n",
        "        self.steps = 0\n",
        "        self.episode = 0\n",
        "        self.rall = 0\n",
        "        self.recent_rlist = deque(maxlen=100)\n",
        "        self.child_conn = child_conn\n",
        "\n",
        "        self.life_done = life_done\n",
        "        self.sticky_action = sticky_action\n",
        "        self.last_action = 0\n",
        "        self.p = p\n",
        "\n",
        "        self.history_size = history_size\n",
        "        self.history = np.zeros([history_size, h, w])\n",
        "        self.h = h\n",
        "        self.w = w\n",
        "\n",
        "        self.reset()\n",
        "\n",
        "    def run(self):\n",
        "        super(MarioEnvironment, self).run()\n",
        "        while True:\n",
        "            action = self.child_conn.recv()\n",
        "            if self.is_render:\n",
        "                self.env.render()\n",
        "\n",
        "            # sticky action\n",
        "            if self.sticky_action:\n",
        "                if np.random.rand() <= self.p:\n",
        "                    action = self.last_action\n",
        "                self.last_action = action\n",
        "\n",
        "            # 4 frame skip\n",
        "            reward = 0.0\n",
        "            done = None\n",
        "            for i in range(4):\n",
        "                obs, r, done, info = self.env.step(action) # mario\n",
        "                if self.is_render:\n",
        "                    self.env.render()\n",
        "                reward += r\n",
        "                if done:\n",
        "                    break\n",
        "\n",
        "            # when Mario loses life, changes the state to the terminal\n",
        "            # state.\n",
        "            if self.life_done:\n",
        "                if self.lives > info['life'] and info['life'] > 0:\n",
        "                    force_done = True\n",
        "                    self.lives = info['life']\n",
        "                else:\n",
        "                    force_done = done\n",
        "                    self.lives = info['life']\n",
        "            else:\n",
        "                force_done = done\n",
        "\n",
        "            # reward range -15 ~ 15\n",
        "            log_reward = reward / 15\n",
        "            self.rall += log_reward\n",
        "\n",
        "            r = int(info.get('flag_get', False))\n",
        "\n",
        "            self.history[:3, :, :] = self.history[1:, :, :]\n",
        "            self.history[3, :, :] = self.pre_proc(obs)\n",
        "\n",
        "            self.steps += 1\n",
        "\n",
        "            if done:\n",
        "                self.recent_rlist.append(self.rall)\n",
        "                print(\n",
        "                    \"[Episode {}({})] Step: {}  Reward: {}  Recent Reward: {}  Stage: {} current x:{}   max x:{}\".format(\n",
        "                        self.episode,\n",
        "                        self.env_idx,\n",
        "                        self.steps,\n",
        "                        self.rall,\n",
        "                        np.mean(\n",
        "                            self.recent_rlist),\n",
        "                        info['stage'],\n",
        "                        info['x_pos'],\n",
        "                        self.max_pos))\n",
        "\n",
        "                self.history = self.reset()\n",
        "\n",
        "            self.child_conn.send([self.history[:, :, :], r, force_done, done, log_reward])\n",
        "\n",
        "    def reset(self):\n",
        "        self.last_action = 0\n",
        "        self.steps = 0\n",
        "        self.episode += 1\n",
        "        self.rall = 0\n",
        "        self.lives = 3\n",
        "        self.stage = 1\n",
        "        self.max_pos = 0\n",
        "        self.get_init_state(self.env.reset())\n",
        "        return self.history[:, :, :]\n",
        "\n",
        "    def pre_proc(self, X):\n",
        "        # grayscaling\n",
        "        x = cv2.cvtColor(X, cv2.COLOR_RGB2GRAY)\n",
        "        # resize\n",
        "        x = cv2.resize(x, (self.h, self.w))\n",
        "\n",
        "        return x\n",
        "\n",
        "    def get_init_state(self, s):\n",
        "        for i in range(self.history_size):\n",
        "            self.history[i, :, :] = self.pre_proc(s)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "id": "mBg3XUCDu00D",
        "outputId": "c3ac7e26-e9b9-4298-aa7f-e5a2fa3e2541"
      },
      "outputs": [],
      "source": [
        "from torch.multiprocessing import Pipe\n",
        "\n",
        "from tensorboardX import SummaryWriter\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def main():\n",
        "    print({section: dict(config[section]) for section in config.sections()})\n",
        "    train_method = default_config['TrainMethod']\n",
        "    env_id = default_config['EnvID']\n",
        "    env_type = default_config['EnvType']\n",
        "\n",
        "    if env_type == 'mario':\n",
        "        env = BinarySpaceToDiscreteSpaceEnv(gym_super_mario_bros.make(env_id), COMPLEX_MOVEMENT)\n",
        "    elif env_type == 'atari':\n",
        "        env = gym.make(env_id, render_mode=\"human\")\n",
        "    else:\n",
        "        raise NotImplementedError\n",
        "    input_size = env.observation_space.shape  # 4\n",
        "    output_size = env.action_space.n  # 2\n",
        "\n",
        "    if 'Breakout' in env_id:\n",
        "        output_size -= 1\n",
        "\n",
        "    env.close()\n",
        "\n",
        "    is_load_model = False\n",
        "    is_render = True\n",
        "    model_path = 'models/{}.model'.format(env_id)\n",
        "    predictor_path = 'models/{}.pred'.format(env_id)\n",
        "    target_path = 'models/{}.target'.format(env_id)\n",
        "\n",
        "    writer = SummaryWriter()\n",
        "\n",
        "    use_cuda = default_config.getboolean('UseGPU')\n",
        "    use_gae = default_config.getboolean('UseGAE')\n",
        "    use_noisy_net = default_config.getboolean('UseNoisyNet')\n",
        "\n",
        "    lam = float(default_config['Lambda'])\n",
        "    num_worker = int(default_config['NumEnv'])\n",
        "\n",
        "    num_step = int(default_config['NumStep'])\n",
        "\n",
        "    ppo_eps = float(default_config['PPOEps'])\n",
        "    epoch = int(default_config['Epoch'])\n",
        "    mini_batch = int(default_config['MiniBatch'])\n",
        "    batch_size = int(num_step * num_worker / mini_batch)\n",
        "    learning_rate = float(default_config['LearningRate'])\n",
        "    entropy_coef = float(default_config['Entropy'])\n",
        "    gamma = float(default_config['Gamma'])\n",
        "    int_gamma = float(default_config['IntGamma'])\n",
        "    clip_grad_norm = float(default_config['ClipGradNorm'])\n",
        "    ext_coef = float(default_config['ExtCoef'])\n",
        "    int_coef = float(default_config['IntCoef'])\n",
        "\n",
        "    sticky_action = default_config.getboolean('StickyAction')\n",
        "    action_prob = float(default_config['ActionProb'])\n",
        "    life_done = default_config.getboolean('LifeDone')\n",
        "\n",
        "    reward_rms = RunningMeanStd()\n",
        "    obs_rms = RunningMeanStd(shape=(1, 1, 84, 84))\n",
        "    pre_obs_norm_step = int(default_config['ObsNormStep'])\n",
        "    discounted_reward = RewardForwardFilter(int_gamma)\n",
        "\n",
        "    agent = RNDAgent\n",
        "\n",
        "    if default_config['EnvType'] == 'atari':\n",
        "        env_type = AtariEnvironment\n",
        "    elif default_config['EnvType'] == 'mario':\n",
        "        env_type = MarioEnvironment\n",
        "    else:\n",
        "        raise NotImplementedError\n",
        "\n",
        "    agent = agent(\n",
        "        input_size,\n",
        "        output_size,\n",
        "        num_worker,\n",
        "        num_step,\n",
        "        gamma,\n",
        "        lam=lam,\n",
        "        learning_rate=learning_rate,\n",
        "        ent_coef=entropy_coef,\n",
        "        clip_grad_norm=clip_grad_norm,\n",
        "        epoch=epoch,\n",
        "        batch_size=batch_size,\n",
        "        ppo_eps=ppo_eps,\n",
        "        use_cuda=use_cuda,\n",
        "        use_gae=use_gae,\n",
        "        use_noisy_net=use_noisy_net\n",
        "    )\n",
        "\n",
        "    if is_load_model:\n",
        "        print('load model...')\n",
        "        if use_cuda:\n",
        "            agent.model.load_state_dict(torch.load(model_path))\n",
        "            agent.rnd.predictor.load_state_dict(torch.load(predictor_path))\n",
        "            agent.rnd.target.load_state_dict(torch.load(target_path))\n",
        "        else:\n",
        "            agent.model.load_state_dict(torch.load(model_path, map_location='cpu'))\n",
        "            agent.rnd.predictor.load_state_dict(torch.load(predictor_path, map_location='cpu'))\n",
        "            agent.rnd.target.load_state_dict(torch.load(target_path, map_location='cpu'))\n",
        "        print('load finished!')\n",
        "\n",
        "    works = []\n",
        "    parent_conns = []\n",
        "    child_conns = []\n",
        "    for idx in range(num_worker):\n",
        "        parent_conn, child_conn = Pipe()\n",
        "        work = env_type(env_id, is_render, idx, child_conn, sticky_action=sticky_action, p=action_prob,\n",
        "                        life_done=life_done)\n",
        "        work.start()\n",
        "        works.append(work)\n",
        "        parent_conns.append(parent_conn)\n",
        "        child_conns.append(child_conn)\n",
        "\n",
        "    states = np.zeros([num_worker, 4, 84, 84])\n",
        "\n",
        "    sample_episode = 0\n",
        "    sample_rall = 0\n",
        "    sample_step = 0\n",
        "    sample_env_idx = 0\n",
        "    sample_i_rall = 0\n",
        "    global_update = 0\n",
        "    global_step = 0\n",
        "\n",
        "    # normalize obs\n",
        "    print('Start to initailize observation normalization parameter.....')\n",
        "    next_obs = []\n",
        "    for step in range(num_step * pre_obs_norm_step):\n",
        "        actions = np.random.randint(0, output_size, size=(num_worker,))\n",
        "\n",
        "        for parent_conn, action in zip(parent_conns, actions):\n",
        "            parent_conn.send(action)\n",
        "\n",
        "        for parent_conn in parent_conns:\n",
        "            s, r, d, rd, lr = parent_conn.recv()\n",
        "            next_obs.append(s[3, :, :].reshape([1, 84, 84]))\n",
        "\n",
        "        if len(next_obs) % (num_step * num_worker) == 0:\n",
        "            next_obs = np.stack(next_obs)\n",
        "            obs_rms.update(next_obs)\n",
        "            next_obs = []\n",
        "    print('End to initalize...')\n",
        "\n",
        "    while True:\n",
        "        total_state, total_reward, total_done, total_next_state, total_action, total_int_reward, total_next_obs, total_ext_values, total_int_values, total_policy, total_policy_np = \\\n",
        "            [], [], [], [], [], [], [], [], [], [], []\n",
        "        global_step += (num_worker * num_step)\n",
        "        global_update += 1\n",
        "\n",
        "        # Step 1. n-step rollout\n",
        "        for _ in range(num_step):\n",
        "            actions, value_ext, value_int, policy = agent.get_action(np.float32(states) / 255.)\n",
        "\n",
        "            for parent_conn, action in zip(parent_conns, actions):\n",
        "                parent_conn.send(action)\n",
        "\n",
        "            next_states, rewards, dones, real_dones, log_rewards, next_obs = [], [], [], [], [], []\n",
        "            for parent_conn in parent_conns:\n",
        "                s, r, d, rd, lr = parent_conn.recv()\n",
        "                next_states.append(s)\n",
        "                rewards.append(r)\n",
        "                dones.append(d)\n",
        "                real_dones.append(rd)\n",
        "                log_rewards.append(lr)\n",
        "                next_obs.append(s[3, :, :].reshape([1, 84, 84]))\n",
        "\n",
        "            next_states = np.stack(next_states)\n",
        "            rewards = np.hstack(rewards)\n",
        "            dones = np.hstack(dones)\n",
        "            real_dones = np.hstack(real_dones)\n",
        "            next_obs = np.stack(next_obs)\n",
        "\n",
        "            # total reward = int reward + ext Reward\n",
        "            intrinsic_reward = agent.compute_intrinsic_reward(\n",
        "                ((next_obs - obs_rms.mean) / np.sqrt(obs_rms.var)).clip(-5, 5))\n",
        "            intrinsic_reward = np.hstack(intrinsic_reward)\n",
        "            sample_i_rall += intrinsic_reward[sample_env_idx]\n",
        "\n",
        "            total_next_obs.append(next_obs)\n",
        "            total_int_reward.append(intrinsic_reward)\n",
        "            total_state.append(states)\n",
        "            total_reward.append(rewards)\n",
        "            total_done.append(dones)\n",
        "            total_action.append(actions)\n",
        "            total_ext_values.append(value_ext)\n",
        "            total_int_values.append(value_int)\n",
        "            total_policy.append(policy)\n",
        "            total_policy_np.append(policy.cpu().numpy())\n",
        "\n",
        "            states = next_states[:, :, :, :]\n",
        "\n",
        "            sample_rall += log_rewards[sample_env_idx]\n",
        "\n",
        "            sample_step += 1\n",
        "            if real_dones[sample_env_idx]:\n",
        "                sample_episode += 1\n",
        "                writer.add_scalar('data/reward_per_epi', sample_rall, sample_episode)\n",
        "                writer.add_scalar('data/reward_per_rollout', sample_rall, global_update)\n",
        "                writer.add_scalar('data/step', sample_step, sample_episode)\n",
        "                sample_rall = 0\n",
        "                sample_step = 0\n",
        "                sample_i_rall = 0\n",
        "\n",
        "        # calculate last next value\n",
        "        _, value_ext, value_int, _ = agent.get_action(np.float32(states) / 255.)\n",
        "        total_ext_values.append(value_ext)\n",
        "        total_int_values.append(value_int)\n",
        "        # --------------------------------------------------\n",
        "\n",
        "        total_state = np.stack(total_state).transpose([1, 0, 2, 3, 4]).reshape([-1, 4, 84, 84])\n",
        "        total_reward = np.stack(total_reward).transpose().clip(-1, 1)\n",
        "        total_action = np.stack(total_action).transpose().reshape([-1])\n",
        "        total_done = np.stack(total_done).transpose()\n",
        "        total_next_obs = np.stack(total_next_obs).transpose([1, 0, 2, 3, 4]).reshape([-1, 1, 84, 84])\n",
        "        total_ext_values = np.stack(total_ext_values).transpose()\n",
        "        total_int_values = np.stack(total_int_values).transpose()\n",
        "        total_logging_policy = np.vstack(total_policy_np)\n",
        "\n",
        "        # Step 2. calculate intrinsic reward\n",
        "        # running mean intrinsic reward\n",
        "        total_int_reward = np.stack(total_int_reward).transpose()\n",
        "        total_reward_per_env = np.array([discounted_reward.update(reward_per_step) for reward_per_step in\n",
        "                                         total_int_reward.T])\n",
        "        mean, std, count = np.mean(total_reward_per_env), np.std(total_reward_per_env), len(total_reward_per_env)\n",
        "        reward_rms.update_from_moments(mean, std ** 2, count)\n",
        "\n",
        "        # normalize intrinsic reward\n",
        "        total_int_reward /= np.sqrt(reward_rms.var)\n",
        "        writer.add_scalar('data/int_reward_per_epi', np.sum(total_int_reward) / num_worker, sample_episode)\n",
        "        writer.add_scalar('data/int_reward_per_rollout', np.sum(total_int_reward) / num_worker, global_update)\n",
        "        # -------------------------------------------------------------------------------------------\n",
        "\n",
        "        # logging Max action probability\n",
        "        writer.add_scalar('data/max_prob', softmax(total_logging_policy).max(1).mean(), sample_episode)\n",
        "\n",
        "        # Step 3. make target and advantage\n",
        "        # extrinsic reward calculate\n",
        "        ext_target, ext_adv = make_train_data(total_reward,\n",
        "                                              total_done,\n",
        "                                              total_ext_values,\n",
        "                                              gamma,\n",
        "                                              num_step,\n",
        "                                              num_worker)\n",
        "\n",
        "        # intrinsic reward calculate\n",
        "        # None Episodic\n",
        "        int_target, int_adv = make_train_data(total_int_reward,\n",
        "                                              np.zeros_like(total_int_reward),\n",
        "                                              total_int_values,\n",
        "                                              int_gamma,\n",
        "                                              num_step,\n",
        "                                              num_worker)\n",
        "\n",
        "        # add ext adv and int adv\n",
        "        total_adv = int_adv * int_coef + ext_adv * ext_coef\n",
        "        # -----------------------------------------------\n",
        "\n",
        "        # Step 4. update obs normalize param\n",
        "        obs_rms.update(total_next_obs)\n",
        "        # -----------------------------------------------\n",
        "\n",
        "        # Step 5. Training!\n",
        "        agent.train_model(np.float32(total_state) / 255., ext_target, int_target, total_action,\n",
        "                          total_adv, ((total_next_obs - obs_rms.mean) / np.sqrt(obs_rms.var)).clip(-5, 5),\n",
        "                          total_policy)\n",
        "\n",
        "        if global_step % (num_worker * num_step * 100) == 0:\n",
        "            print('Now Global Step :{}'.format(global_step))\n",
        "            torch.save(agent.model.state_dict(), model_path)\n",
        "            torch.save(agent.rnd.predictor.state_dict(), predictor_path)\n",
        "            torch.save(agent.rnd.target.state_dict(), target_path)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyPTbPrYO8ks90jYaDRMOtlt",
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
